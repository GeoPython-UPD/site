{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Least squares regressions\n",
    "\n",
    "```{attention}\n",
    "\n",
    "Enrolled students using DILNET may use the CS JupyterHub.<br/>\n",
    "<a href=\"http://jhub.science.upd.edu.ph/hub/user-redirect/git-pull?repo=https://github.com/GeoPython-UPD/notebooks&urlpath=lab/tree/notebooks/L9/least-squares.ipynb+&branch=main\"><img src=\"https://img.shields.io/badge/Launch-CS_Hub-blue\" alt=\"Launch - CS Hub\"></a>\n",
    "\n",
    "Follow the lesson and fill in your notebooks using Binder.<br/>\n",
    "<a href=\"https://mybinder.org/v2/gh/GeoPython-UPD/notebooks/main?labpath=L8/least-squares.ipynb\"><img alt=\"Binder badge\" src=\"https://img.shields.io/badge/launch-binder-red.svg\" style=\"vertical-align:text-bottom\"></a>\n",
    "```\n",
    "\n",
    "*Least squares regressions* are a common way of determining whether two values are *linearly* related to one an other. In other words, this is a method to determine whether a line is a good \"fit\" to some measured values. Not all data should be expected to be fit well by a line, but linear regressions are a powerful method for determining cases when two variables are directly related to one another. A common example might be the temperature at which magma erupts versus the SiO<sub>2</sub> content of the magma, as shown below in Figure 2.1.\n",
    "\n",
    "![Magma temperatures](img/magma-temps.png)\n",
    "\n",
    "_**Figure 2.1**. Eruption temperatures of magmas as a function of their SiO<sub>2</sub> content with a linear regression line. Source: Figure 16.1 from [McKillup and Dyar, 2010](http://www.cambridge.org/fi/academic/subjects/earth-and-environmental-science/earth-science-general-interest/geostatistics-explained-introductory-guide-earth-scientists?format=HB&isbn=9780521763226)._\n",
    "\n",
    "The general idea with calculating a *linear regression* is that we want to find the equation of a line that best fits some $x$-$y$ data, such as temperature and SiO<sub>2</sub> content in the example above. To do this, we first need to recall the equation for a line:\n",
    "\n",
    "$$\\large y = A + B x$$\n",
    "\n",
    "where $x$ and $y$ are the coordinates of the data points, $A$ is the $y$-intercept, and $B$ is the slope of the line.\n",
    "\n",
    "Thus, in order to calculate a \"best fit\" line to some data, we will need to determine the values of the constants $A$ and $B$. Consider the example below in which $A$ and $B$ are known. If we make the rather common assumption that the uncertainties for the values on the $x$ axis are negligible compared to the uncertainties along the $y$ axis, we can say:\n",
    "\n",
    "$$\\large (\\mathrm{true~value~of~}y_{i}) = A + B x_{i}$$\n",
    "\n",
    "Thus, it is possible to find the value of $y$ for two linearly related values when $A$ and $B$ are known.\n",
    "\n",
    "Finding the values of $A$ and $B$ then for the case of a linear regression to some $x$-$y$ data is fairly straightforward, though it does involve a bit of algebra. For our purposes, I'll refer you to Chapter 8 of [Taylor, 1997](http://www.uscibooks.com/taylornb.htm) for a complete explanation of how to find $A$ and $B$, and simply present the relevant equations below. The value of the $y$-intercept can be found using\n",
    "\n",
    "$$\\large A = \\frac{\\sum{x^{2}} \\sum{y} - \\sum{x} \\sum{xy}}{\\Delta}$$\n",
    "\n",
    "where $x$ is the $i$th data point plotted on the $x$-axis, $y$ is the $i$th data point plotted on the $y$-axis, and $\\Delta$ is defined below.\n",
    "\n",
    "The line slope can be found using\n",
    "\n",
    "$$\\large B = \\frac{N \\sum{xy} - \\sum{x} \\sum {y}}{\\Delta}$$\n",
    "\n",
    "where $N$ is the number of values in the regression.\n",
    "\n",
    "And the value of $\\Delta$ is\n",
    "\n",
    "$$\\large \\Delta = N \\sum{x^{2}} - \\left( \\sum{x} \\right)^{2}$$\n",
    "\n",
    "With the equations above, you are now able to calculate *unweighted* regression lines, the best-fit lines to some $x$-$y$ data in which the uncertainties in the measurements are not considered to influence the fit of the line. It is also possible to fit regression lines that consider the variable uncertainties in the data, referred to as *weighted regressions*, but will will not consider that type of regression for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class demonstration space\n",
    "\n",
    "The cell below can be used for following live demonstrations during the class lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding done during class time goes below\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create Numpy arrays with toy dataset\n",
    "ages = np.array([34, 22, 22, 27, 31, 29, 22, 23, 20]) # ka\n",
    "elev = np.array([210, 120, 120, 210, 150, 120, 120, 120, 90]) # meters\n",
    "\n",
    "delta = (len(ages) * (ages**2).sum()) - (ages.sum()**2)\n",
    "A = (((ages**2).sum() * elev.sum()) - (ages.sum() * (ages*elev).sum())) / delta\n",
    "B = ((len(ages)*(ages*elev).sum()) - (ages.sum() * elev.sum())) / delta\n",
    "\n",
    "print(f\"Slope: {B}\")\n",
    "print(f\"Intercept: {A}\")\n",
    "\n",
    "# Initialize figure and axes\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "y1 = A + B * ages.min()\n",
    "y2 = A + B * ages.max()\n",
    "x1 = ages.min()\n",
    "x2 = ages.max()\n",
    "\n",
    "# plot data\n",
    "ax.scatter(ages, elev)\n",
    "ax.plot([x1, x2], [y1, y2])\n",
    "ax.set_xlabel('Ages, ka')\n",
    "ax.set_ylabel('Elevation, m')\n",
    "ax.text(20,200, f\"Slope/Uplift rate: {B:.3f} m/ka\")\n",
    "ax.text(20,190, f\"Intercept: {A:.3f}\")\n",
    "\n",
    "# initialize summing variables...\n",
    "topsum = 0\n",
    "bottomsumx = 0\n",
    "bottomsumy = 0\n",
    "\n",
    "for i in range(len(ages)):\n",
    "    topsum = topsum + (ages[i] - ages.mean()) * (elev[i] - elev.mean())\n",
    "    bottomsumx = bottomsumx + (ages[i] - ages.mean()) ** 2\n",
    "    bottomsumy = bottomsumy + (elev[i] - elev.mean()) ** 2\n",
    "\n",
    "# Calculate r\n",
    "r = topsum / np.sqrt(bottomsumx * bottomsumy)\n",
    "\n",
    "# print out correlation\n",
    "print(f\"Correlation coefficient: {r:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
